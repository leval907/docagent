# Crawl4AI Integration Guide

## –ß—Ç–æ —Ç–∞–∫–æ–µ Crawl4AI?

**Crawl4AI** - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π web-crawler —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π JavaScript-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è LLM –∏ RAG —Å–∏—Å—Ç–µ–º.

### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞

‚úÖ **JavaScript-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥** —á–µ—Ä–µ–∑ Playwright  
‚úÖ **SPA Support** (React, Vue, VitePress)  
‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ** –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö  
‚úÖ **Markdown –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è** –∏–∑ –ª—é–±–æ–≥–æ HTML  
‚úÖ **–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫** –¥–ª—è —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–≥–æ –æ–±—Ö–æ–¥–∞  
‚úÖ **Stealth mode** –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã  

### vs markdown-crawler

| –§—É–Ω–∫—Ü–∏—è | Crawl4AI | markdown-crawler |
|---------|----------|------------------|
| JavaScript | ‚úÖ | ‚ùå |
| SPA —Å–∞–π—Ç—ã | ‚úÖ | ‚ùå |
| –°–∫–æ—Ä–æ—Å—Ç—å | üê¢ –ú–µ–¥–ª–µ–Ω–Ω–µ–µ | üöÄ –ë—ã—Å—Ç—Ä–µ–µ |
| –ù–∞–¥—ë–∂–Ω–æ—Å—Ç—å | ‚úÖ –í—ã—Å–æ–∫–∞—è | ‚ö†Ô∏è –°—Ä–µ–¥–Ω—è—è |
| –†–∞–∑–º–µ—Ä | üì¶ ~400 MB | üì¶ ~10 MB |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Crawl4AI –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞

```bash
# –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å venv
.venv\Scripts\Activate.ps1  # Windows
source .venv/bin/activate   # Linux/Mac

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Crawl4AI
pip install crawl4ai

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –±—Ä–∞—É–∑–µ—Ä—ã Playwright
playwright install
```

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### –ë–∞–∑–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä

```python
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def crawl_page():
    browser_config = BrowserConfig(headless=True, verbose=True)
    crawl_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://nocodb.com/docs/product-docs",
            config=crawl_config
        )
        
        if result.success:
            print(f"Title: {result.metadata['title']}")
            print(f"Markdown: {result.markdown[:500]}...")
            print(f"Links: {len(result.links['internal'])} internal")

asyncio.run(crawl_page())
```

### –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥

```python
async def crawl_recursive(start_url, max_depth=3, max_pages=50):
    visited = set()
    to_visit = [(start_url, 0)]
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        while to_visit and len(visited) < max_pages:
            url, depth = to_visit.pop(0)
            
            if url in visited or depth > max_depth:
                continue
            
            visited.add(url)
            result = await crawler.arun(url=url)
            
            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å result.markdown
            # –î–æ–±–∞–≤–∏—Ç—å internal links –≤ to_visit
```

## DocAgent Integration

### –°–∫—Ä–∏–ø—Ç crawler_crawl4ai.py

–ü–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π crawler —Å:
- –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–º –æ–±—Ö–æ–¥–æ–º
- –§–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –¥–æ–º–µ–Ω—É
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
- YAML –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
- JSON –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π

**–ó–∞–ø—É—Å–∫**:
```bash
python scripts/crawler_crawl4ai.py --app nocodb
```

### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

–í `config/sources.yaml` –¥–æ–±–∞–≤–∏—Ç—å:
```yaml
apps:
  nocodb:
    name: "NocoDB"
    url: "https://nocodb.com/docs/product-docs"
    depth: 2           # –ì–ª—É–±–∏–Ω–∞ —Ä–µ–∫—É—Ä—Å–∏–∏
    max_pages: 50      # –ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü
    enabled: true
```

### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

```
knowledge_base/nocodb/
‚îú‚îÄ‚îÄ docs-product-docs.md
‚îú‚îÄ‚îÄ docs-product-docs-bases.md
‚îú‚îÄ‚îÄ docs-product-docs-bases-create-base.md
‚îú‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ index.json
```

–ö–∞–∂–¥—ã–π —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç:
```markdown
---
title: "Create Base"
source: "https://nocodb.com/docs/..."
crawled_at: "2025-11-09T00:26:31"
file_hash: "01768ce..."
word_count: 356
---

# Create base
...
```

## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### NocoDB (–ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ ‚úÖ)

```bash
python scripts/crawler_crawl4ai.py --app nocodb
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç**:
- ‚úÖ 50 —Å—Ç—Ä–∞–Ω–∏—Ü
- ‚úÖ 26,988 —Å–ª–æ–≤
- ‚úÖ ~2 –º–∏–Ω—É—Ç—ã
- ‚úÖ –í—Å–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ

### –î—Ä—É–≥–∏–µ —Å–∞–π—Ç—ã

**–†–∞–±–æ—Ç–∞–µ—Ç**:
- ‚úÖ FastAPI (https://fastapi.tiangolo.com/)
- ‚úÖ Requesty AI (https://docs.requesty.ai/)
- ‚úÖ LangChain (https://python.langchain.com/)

**–ü—Ä–æ–±–ª–µ–º—ã**:
- ‚ö†Ô∏è DB-GPT (SSL –æ—à–∏–±–∫–∏ –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Ö–æ—Å—Ç–∞—Ö)

## –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –°–∫–æ—Ä–æ—Å—Ç—å

```python
# –ë—ã—Å—Ç—Ä–µ–µ (–±–µ–∑ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞)
crawl_config = CrawlerRunConfig(
    wait_for="none",
    css_selector=None
)

# –ú–µ–¥–ª–µ–Ω–Ω–µ–µ (–ø–æ–ª–Ω—ã–π —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥)
crawl_config = CrawlerRunConfig(
    wait_for="networkidle",
    delay_before_return_html=2.0
)
```

### –ó–∞–¥–µ—Ä–∂–∫–∏

```python
# –í crawler_crawl4ai.py:
await asyncio.sleep(0.5)  # 500ms –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
```

–ú–æ–∂–Ω–æ —É–º–µ–Ω—å—à–∏—Ç—å –¥–æ 0.1-0.2 —Å–µ–∫—É–Ω–¥, –Ω–æ —Ä–∏—Å–∫ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏.

### –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º

**–ù–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ** (–Ω–æ –≤–æ–∑–º–æ–∂–Ω–æ):
```python
# –°–æ–∑–¥–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ AsyncWebCrawler
# –†–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç—å URLs –º–µ–∂–¥—É –Ω–∏–º–∏
# –°–æ–±—Ä–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
```

## –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

### –¢–∏–ø–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã

**1. SSL Errors**
```
SSLError: [SSL: UNEXPECTED_EOF_WHILE_READING]
```
**–†–µ—à–µ–Ω–∏–µ**: –î–æ–±–∞–≤–∏—Ç—å `verify_ssl=False` –≤ BrowserConfig

**2. Timeout**
```
TimeoutError: Page didn't load in time
```
**–†–µ—à–µ–Ω–∏–µ**: –£–≤–µ–ª–∏—á–∏—Ç—å `page_timeout` –≤ CrawlerRunConfig

**3. No links found**
```
DEBUG: Found 0 child URLs
```
**–ü—Ä–æ–≤–µ—Ä–∏—Ç—å**: –ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å base_url –∏ domain matching

### –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
from loguru import logger

logger.add("logs/crawler.log", rotation="10 MB")
logger.info("Crawling {url}", url=url)
```

## –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

### –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö

```python
# –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
result = await crawler.arun(
    url=url,
    extraction_strategy=JsonCssExtractionStrategy(
        schema={
            "title": "h1",
            "content": "article",
            "links": "a[href]"
        }
    )
)
```

### –°–∫—Ä–∏–Ω—à–æ—Ç—ã

```python
result = await crawler.arun(
    url=url,
    screenshot=True
)
# result.screenshot —Å–æ–¥–µ—Ä–∂–∏—Ç base64
```

### –ö–∞—Å—Ç–æ–º–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã

```python
crawl_config = CrawlerRunConfig(
    css_selector="article.documentation",
    excluded_tags=['nav', 'footer', 'aside']
)
```

## Docker

```dockerfile
FROM python:3.11-slim

RUN pip install crawl4ai playwright
RUN playwright install chromium
RUN playwright install-deps

COPY . /app
WORKDIR /app

CMD ["python", "scripts/crawler_crawl4ai.py", "--app", "nocodb"]
```

## FAQ

**Q: –ü–æ—á–µ–º—É —Ç–∞–∫ –º–µ–¥–ª–µ–Ω–Ω–æ?**  
A: Playwright –∑–∞–ø—É—Å–∫–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π –±—Ä–∞—É–∑–µ—Ä. ~1-2 —Å–µ–∫/—Å—Ç—Ä–∞–Ω–∏—Ü–∞ —ç—Ç–æ –Ω–æ—Ä–º–∞.

**Q: –ú–æ–∂–Ω–æ –ª–∏ —É—Å–∫–æ—Ä–∏—Ç—å?**  
A: –î–∞, —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º (–Ω–µ—Å–∫–æ–ª—å–∫–æ crawler'–æ–≤) –∏–ª–∏ headless —Ä–µ–∂–∏–º.

**Q: –†–∞–±–æ—Ç–∞–µ—Ç –ª–∏ —Å API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π?**  
A: –î–∞, –µ—Å–ª–∏ –æ–Ω–∞ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –∫–∞–∫ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã.

**Q: –ö–∞–∫ –æ–±–æ–π—Ç–∏ Cloudflare?**  
A: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `stealth_mode=True` –∏ –¥–æ–±–∞–≤—å—Ç–µ –∑–∞–¥–µ—Ä–∂–∫–∏.

**Q: –ù—É–∂–µ–Ω –ª–∏ markdown-crawler?**  
A: –ù–µ—Ç, Crawl4AI –µ–≥–æ –∑–∞–º–µ–Ω—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é.

## –°—Å—ã–ª–∫–∏

- üìö [Crawl4AI Docs](https://docs.crawl4ai.com)
- üêô [GitHub](https://github.com/unclecode/crawl4ai)
- üí¨ [Discord](https://discord.gg/crawl4ai)

---

**–ê–≤—Ç–æ—Ä**: DocAgent Team  
**–î–∞—Ç–∞**: 09.11.2025  
**–í–µ—Ä—Å–∏—è**: 2.0
