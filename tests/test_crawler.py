#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã DocAgent –ø–∞—Ä—Å–µ—Ä–∞ (Crawl4AI)
"""

import sys
import subprocess
from pathlib import Path
from loguru import logger

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger.remove()
logger.add(sys.stderr, format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>")


def run_command(cmd: list, description: str):
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –∫–æ–º–∞–Ω–¥—É"""
    logger.info(f"\n{'='*60}")
    logger.info(f"üöÄ {description}")
    logger.info(f"{'='*60}")
    logger.info(f"Command: {' '.join(cmd)}\n")
    
    try:
        result = subprocess.run(
            cmd,
            check=True,
            capture_output=False,
            text=True
        )
        logger.success(f"‚úÖ {description} - SUCCESS\n")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"‚ùå {description} - FAILED")
        logger.error(f"Error: {e}\n")
        return False
    except Exception as e:
        logger.error(f"‚ùå Unexpected error: {e}\n")
        return False


def check_crawl4ai():
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ Crawl4AI"""
    try:
        import crawl4ai
        logger.success(f"‚úÖ Crawl4AI {crawl4ai.__version__} found")
        return True
    except ImportError:
        logger.warning("‚ö†Ô∏è Crawl4AI not found!")
        logger.info("\nPlease install it:")
        logger.info("  pip install crawl4ai")
        logger.info("  playwright install")
        return False


def test_crawler():
    """–¢–µ—Å—Ç Crawl4AI crawler"""
    logger.info("\n" + "="*60)
    logger.info("üìã TEST 1: Crawl4AI Crawler")
    logger.info("="*60)
    
    # –¢–µ—Å—Ç —Å nocodb
    logger.info("\n1Ô∏è‚É£ Crawl NocoDB documentation:")
    run_command(
        [sys.executable, "scripts/crawler_crawl4ai.py", "--app", "nocodb"],
        "Crawl nocodb"
    )


def test_postprocessor():
    """–¢–µ—Å—Ç postprocessor"""
    logger.info("\n" + "="*60)
    logger.info("üìã TEST 2: Postprocessor")
    logger.info("="*60)
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –µ—Å—Ç—å –ª–∏ —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    kb_dir = Path("knowledge_base/nocodb")
    
    if not kb_dir.exists() or not list(kb_dir.glob("*.md")):
        logger.warning("‚ö†Ô∏è No markdown files found for testing")
        logger.info("Run crawler first:")
        logger.info("  python scripts/crawler_crawl4ai.py --app nocodb")
        return False
    
    # –ó–∞–ø—É—Å—Ç–∏—Ç—å postprocessor
    run_command(
        [sys.executable, "scripts/postprocess.py", "--app", "nocodb"],
        "Add YAML metadata"
    )
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    md_files = list(kb_dir.glob("*.md"))
    if md_files:
        test_file = md_files[0]
        logger.info(f"\nüìÑ Sample file: {test_file.name}")
        
        with open(test_file, 'r', encoding='utf-8') as f:
            first_lines = ''.join(f.readlines()[:20])
        
        logger.info("First 20 lines:")
        logger.info("-" * 60)
        print(first_lines)
        logger.info("-" * 60)
    
    return True


def test_indexer():
    """–¢–µ—Å—Ç indexer"""
    logger.info("\n" + "="*60)
    logger.info("üìã TEST 3: Indexer")
    logger.info("="*60)
    
    # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å
    run_command(
        [sys.executable, "scripts/build_index.py", "--app", "nocodb"],
        "Build app index"
    )
    
    # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å
    run_command(
        [sys.executable, "scripts/build_index.py", "--all"],
        "Build global index"
    )
    
    return True


def full_pipeline_test():
    """–ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    logger.info("\n" + "="*60)
    logger.info("üöÄ FULL PIPELINE TEST")
    logger.info("="*60)
    
    steps = [
        ("Crawler", [sys.executable, "scripts/crawler_crawl4ai.py", "--app", "nocodb"]),
        ("Postprocessor", [sys.executable, "scripts/postprocess.py", "--app", "nocodb"]),
        ("Indexer", [sys.executable, "scripts/build_index.py", "--app", "nocodb"]),
    ]
    
    results = []
    for step_name, cmd in steps:
        success = run_command(cmd, f"Step: {step_name}")
        results.append((step_name, success))
    
    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç
    logger.info("\n" + "="*60)
    logger.info("üìä PIPELINE TEST RESULTS")
    logger.info("="*60)
    
    for step_name, success in results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        logger.info(f"{status} - {step_name}")
    
    total_success = all(s for _, s in results)
    
    if total_success:
        logger.success("\nüéâ All tests passed!")
    else:
        logger.error("\n‚ùå Some tests failed")
    
    return total_success


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞"""
    logger.info("üß™ DocAgent Test Suite (Crawl4AI)")
    logger.info("="*60)
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å Crawl4AI
    if not check_crawl4ai():
        logger.error("\n‚ùå Setup incomplete. Please install Crawl4AI first.")
        logger.info("  pip install crawl4ai")
        logger.info("  playwright install")
        return
    
    # –ú–µ–Ω—é –≤—ã–±–æ—Ä–∞
    logger.info("\nSelect test mode:")
    logger.info("  1 - Test crawler only")
    logger.info("  2 - Test postprocessor only")
    logger.info("  3 - Test indexer only")
    logger.info("  4 - Full pipeline test (recommended)")
    logger.info("  q - Quit")
    
    choice = input("\nEnter your choice [1-4, q]: ").strip()
    
    if choice == '1':
        test_crawler()
    elif choice == '2':
        test_postprocessor()
    elif choice == '3':
        test_indexer()
    elif choice == '4':
        full_pipeline_test()
    elif choice.lower() == 'q':
        logger.info("Bye! üëã")
    else:
        logger.warning("Invalid choice")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è Interrupted by user")
    except Exception as e:
        logger.error(f"‚ùå Fatal error: {e}")
