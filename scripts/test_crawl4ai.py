#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ Crawl4AI
"""
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

async def test_crawl():
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –±—Ä–∞—É–∑–µ—Ä–∞
    browser_config = BrowserConfig(
        headless=True,
        verbose=True
    )
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫—Ä–∞—É–ª–µ—Ä–∞
    crawl_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        markdown_generator=None,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
    )
    
    # URL –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    url = "https://nocodb.com/docs/product-docs"
    
    print(f"üï∏Ô∏è Crawling: {url}")
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url=url,
            config=crawl_config
        )
        
        if result.success:
            print(f"‚úÖ Success!")
            print(f"üìÑ Title: {result.metadata.get('title', 'N/A')}")
            print(f"üìù Markdown length: {len(result.markdown)} characters")
            print(f"üîó Links found: {len(result.links.get('internal', []))} internal, {len(result.links.get('external', []))} external")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            output_file = "D:/docs/DocAgent/knowledge_base/test/nocodb_test.md"
            import os
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(f"# {result.metadata.get('title', 'Document')}\n\n")
                f.write(f"Source: {url}\n\n")
                f.write(result.markdown)
            
            print(f"üíæ Saved to: {output_file}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏
            internal_links = result.links.get('internal', [])
            if internal_links:
                print(f"\nüîó First 10 internal links:")
                for link in internal_links[:10]:
                    print(f"   - {link}")
        else:
            print(f"‚ùå Failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(test_crawl())
