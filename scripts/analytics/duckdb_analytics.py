#!/usr/bin/env python3
"""
DuckDB Analytics & Dataset Integration
–ò–º–ø–æ—Ä—Ç —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ–∏—Å–∫ —Å–≤—è–∑–µ–π
"""

import duckdb
import os
from pathlib import Path


class DuckDBAnalytics:
    def __init__(self, db_path: str = "knowledge_base/duckdb/analytics.duckdb"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.conn = duckdb.connect(str(self.db_path))
        self._install_extensions()
    
    def _install_extensions(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π DuckDB"""
        print("üì¶ Installing DuckDB extensions...")
        
        # –î–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏
        extensions = [
            'spatial',      # Excel, spatial data
            'json',         # JSON files
            'parquet',      # Parquet (comes by default)
            'postgres',     # PostgreSQL integration
            'httpfs',       # Read from HTTP/S3
        ]
        
        for ext in extensions:
            try:
                self.conn.execute(f"INSTALL {ext};")
                self.conn.execute(f"LOAD {ext};")
                print(f"  ‚úÖ {ext}")
            except Exception as e:
                print(f"  ‚ö†Ô∏è  {ext}: {e}")
    
    def import_excel(self, file_path: str, table_name: str, sheet: str = None):
        """
        –ò–º–ø–æ—Ä—Ç Excel —Ñ–∞–π–ª–∞
        
        Example:
            analytics.import_excel('data.xlsx', 'sales', sheet='2024')
        """
        print(f"\nüìä Importing Excel: {file_path}")
        
        sheet_clause = f", sheet='{sheet}'" if sheet else ""
        
        query = f"""
            CREATE OR REPLACE TABLE {table_name} AS 
            SELECT * FROM st_read('{file_path}'{sheet_clause});
        """
        
        self.conn.execute(query)
        count = self.conn.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
        print(f"  ‚úÖ Imported {count} rows into {table_name}")
        
        # –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        self.show_table_info(table_name)
    
    def import_csv(self, file_path: str, table_name: str, delimiter: str = ','):
        """–ò–º–ø–æ—Ä—Ç CSV"""
        print(f"\nüìä Importing CSV: {file_path}")
        
        query = f"""
            CREATE OR REPLACE TABLE {table_name} AS 
            SELECT * FROM read_csv('{file_path}', 
                delim='{delimiter}', 
                header=true, 
                auto_detect=true
            );
        """
        
        self.conn.execute(query)
        count = self.conn.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
        print(f"  ‚úÖ Imported {count} rows into {table_name}")
        
        self.show_table_info(table_name)
    
    def import_json(self, file_path: str, table_name: str):
        """–ò–º–ø–æ—Ä—Ç JSON (–º–∞—Å—Å–∏–≤ –æ–±—ä–µ–∫—Ç–æ–≤)"""
        print(f"\nüìä Importing JSON: {file_path}")
        
        query = f"""
            CREATE OR REPLACE TABLE {table_name} AS 
            SELECT * FROM read_json_auto('{file_path}');
        """
        
        self.conn.execute(query)
        count = self.conn.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
        print(f"  ‚úÖ Imported {count} rows into {table_name}")
        
        self.show_table_info(table_name)
    
    def import_parquet(self, file_path: str, table_name: str):
        """–ò–º–ø–æ—Ä—Ç Parquet"""
        print(f"\nüìä Importing Parquet: {file_path}")
        
        query = f"""
            CREATE OR REPLACE TABLE {table_name} AS 
            SELECT * FROM read_parquet('{file_path}');
        """
        
        self.conn.execute(query)
        count = self.conn.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
        print(f"  ‚úÖ Imported {count} rows into {table_name}")
        
        self.show_table_info(table_name)
    
    def connect_postgres(self, pg_url: str = None):
        """
        –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL
        
        Example:
            analytics.connect_postgres('postgresql://user:pass@localhost/docagent')
        """
        if not pg_url:
            pg_url = "postgresql://analytics_user:analytics_secure_2025@localhost:5432/docagent"
        
        print(f"\nüîó Connecting to PostgreSQL...")
        
        self.conn.execute(f"""
            ATTACH '{pg_url}' AS pg (TYPE POSTGRES);
        """)
        
        # –ü–æ–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
        tables = self.conn.execute("""
            SELECT table_name 
            FROM information_schema.tables 
            WHERE table_schema = 'pg'
        """).fetchall()
        
        print(f"  ‚úÖ Connected! Available tables:")
        for table in tables:
            print(f"     - pg.{table[0]}")
    
    def show_table_info(self, table_name: str):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü—ã"""
        print(f"\n  üìã Table structure for {table_name}:")
        
        columns = self.conn.execute(f"DESCRIBE {table_name}").fetchall()
        for col in columns:
            print(f"     {col[0]:20s} {col[1]}")
    
    def list_tables(self):
        """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü"""
        print("\nüìö Available tables:")
        
        tables = self.conn.execute("""
            SELECT table_name, 
                   (SELECT COUNT(*) FROM information_schema.columns 
                    WHERE table_name = t.table_name) as column_count
            FROM information_schema.tables t
            WHERE table_schema = 'main'
            ORDER BY table_name
        """).fetchall()
        
        for table, cols in tables:
            count = self.conn.execute(f"SELECT COUNT(*) FROM {table}").fetchone()[0]
            print(f"  üìä {table:30s} {count:>8,} rows, {cols} columns")
    
    def find_common_columns(self, table1: str, table2: str):
        """–ù–∞–π—Ç–∏ –æ–±—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏"""
        print(f"\nüîç Finding common columns: {table1} ‚Üî {table2}")
        
        result = self.conn.execute(f"""
            SELECT DISTINCT column_name
            FROM information_schema.columns
            WHERE table_name = '{table1}'
            INTERSECT
            SELECT DISTINCT column_name
            FROM information_schema.columns
            WHERE table_name = '{table2}'
        """).fetchall()
        
        if result:
            print(f"  ‚úÖ Found {len(result)} common columns:")
            for col in result:
                print(f"     - {col[0]}")
        else:
            print(f"  ‚ùå No common columns found")
        
        return [r[0] for r in result]
    
    def suggest_joins(self, table1: str, table2: str):
        """–ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω—ã–µ JOIN'—ã –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏"""
        common_cols = self.find_common_columns(table1, table2)
        
        if not common_cols:
            print("\nüí° Trying fuzzy matching...")
            # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ –∫–æ–ª–æ–Ω–∫–∏
            result = self.conn.execute(f"""
                WITH t1_cols AS (
                    SELECT column_name FROM information_schema.columns 
                    WHERE table_name = '{table1}'
                ),
                t2_cols AS (
                    SELECT column_name FROM information_schema.columns 
                    WHERE table_name = '{table2}'
                )
                SELECT 
                    t1.column_name as col1,
                    t2.column_name as col2,
                    levenshtein(t1.column_name, t2.column_name) as distance
                FROM t1_cols t1, t2_cols t2
                WHERE levenshtein(t1.column_name, t2.column_name) <= 3
                ORDER BY distance
                LIMIT 5
            """).fetchall()
            
            if result:
                print("\n  üéØ Potential fuzzy matches:")
                for col1, col2, dist in result:
                    print(f"     {col1} ‚âà {col2} (distance: {dist})")
        else:
            print("\nüí° Suggested JOIN queries:")
            for col in common_cols:
                print(f"""
    SELECT *
    FROM {table1} t1
    JOIN {table2} t2 ON t1.{col} = t2.{col}
    LIMIT 10;
                """)
    
    def analyze_relationships(self):
        """–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –≤—Å–µ–º–∏ —Ç–∞–±–ª–∏—Ü–∞–º–∏"""
        print("\nüîç Analyzing relationships between tables...")
        
        tables = self.conn.execute("""
            SELECT table_name 
            FROM information_schema.tables 
            WHERE table_schema = 'main'
        """).fetchall()
        
        tables = [t[0] for t in tables]
        
        print(f"\nüìä Found {len(tables)} tables")
        
        relationships = []
        for i, t1 in enumerate(tables):
            for t2 in tables[i+1:]:
                common = self.find_common_columns(t1, t2)
                if common:
                    relationships.append((t1, t2, common))
        
        if relationships:
            print(f"\n‚úÖ Found {len(relationships)} potential relationships:")
            for t1, t2, cols in relationships:
                print(f"\n  {t1} ‚Üî {t2}")
                print(f"    Common columns: {', '.join(cols)}")
        else:
            print("\n‚ùå No direct relationships found")
    
    def query(self, sql: str):
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π SQL –∑–∞–ø—Ä–æ—Å"""
        print(f"\nüîç Executing query:")
        print(f"   {sql[:100]}...")
        
        result = self.conn.execute(sql).fetchall()
        
        # –ü–æ–ª—É—á–∏—Ç—å –∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫
        description = self.conn.description
        if description:
            cols = [d[0] for d in description]
            print(f"\n  üìä Results ({len(result)} rows):")
            
            # –ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫
            for row in result[:10]:
                print("  ", dict(zip(cols, row)))
        
        return result
    
    def export_to_excel(self, query: str, output_file: str):
        """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ Excel"""
        print(f"\nüì§ Exporting to Excel: {output_file}")
        
        self.conn.execute(f"""
            COPY ({query}) 
            TO '{output_file}' 
            WITH (FORMAT GDAL, DRIVER 'XLSX');
        """)
        
        print(f"  ‚úÖ Exported to {output_file}")
    
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ"""
        self.conn.close()


def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
    
    analytics = DuckDBAnalytics()
    
    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  DuckDB Analytics - Dataset Integration                  ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Available commands:
  analytics.import_excel('file.xlsx', 'table_name')
  analytics.import_csv('file.csv', 'table_name')
  analytics.import_json('file.json', 'table_name')
  analytics.list_tables()
  analytics.find_common_columns('table1', 'table2')
  analytics.suggest_joins('table1', 'table2')
  analytics.analyze_relationships()
  analytics.query("SELECT * FROM table LIMIT 10")
  analytics.export_to_excel("SELECT * FROM table", 'output.xlsx')
  analytics.connect_postgres()  # –ü–æ–¥–∫–ª—é—á–∏—Ç—å PostgreSQL

Example workflow:
  1. Import multiple Excel/CSV files
  2. Use analyze_relationships() to find connections
  3. Create JOIN queries based on suggestions
  4. Export results to Excel for reporting
    """)
    
    # –ü—Ä–∏–º–µ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –µ—Å–ª–∏ –µ—Å—Ç—å —Ñ–∞–π–ª—ã
    kb_dir = Path("knowledge_base/duckdb")
    if kb_dir.exists():
        print("\nüîç Scanning knowledge_base/duckdb/ for data files...")
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        for file in kb_dir.glob("*.csv"):
            table_name = file.stem.replace('-', '_').replace(' ', '_')
            try:
                analytics.import_csv(str(file), table_name)
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Failed to import {file.name}: {e}")
        
        for file in kb_dir.glob("*.json"):
            table_name = file.stem.replace('-', '_').replace(' ', '_')
            try:
                analytics.import_json(str(file), table_name)
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Failed to import {file.name}: {e}")
        
        # –ü–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ
        analytics.list_tables()
        
        # –ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π
        analytics.analyze_relationships()
    
    return analytics


if __name__ == "__main__":
    analytics = main()
    
    # –û—Å—Ç–∞–≤–∏—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É—é —Å–µ—Å—Å–∏—é
    print("\nüí° DuckDB analytics object is available as 'analytics'")
    print("   Type: help(analytics) for more info")
