#!/usr/bin/env python3
"""
DocAgent Crawler using Crawl4AI
Supports JavaScript-rendered sites
"""
import asyncio
import hashlib
import os
import re
import yaml
from pathlib import Path
from typing import Set, List, Dict
from urllib.parse import urlparse, urljoin
from datetime import datetime

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from loguru import logger
import click


class DocAgentCrawl4AI:
    def __init__(self, config_path: str = "config/sources.yaml"):
        self.config_path = config_path
        self.config = self._load_config()
        self.base_output_dir = Path(self.config.get('global', {}).get('output_base_dir', './knowledge_base'))
        
    def _load_config(self) -> dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ YAML"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _get_enabled_apps(self) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π"""
        apps = []
        for app_id, app_config in self.config.get('apps', {}).items():
            if app_config.get('enabled', False):
                apps.append({
                    'id': app_id,
                    **app_config
                })
        return apps
    
    async def crawl_recursive(
        self, 
        start_url: str, 
        base_url: str,
        output_dir: Path,
        max_depth: int = 3,
        max_pages: int = 100
    ) -> Dict[str, str]:
        """
        –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥ —Å–∞–π—Ç–∞
        
        Returns:
            Dict[url, markdown_content]
        """
        visited: Set[str] = set()
        to_visit: List[tuple] = [(start_url, 0)]  # (url, depth)
        results: Dict[str, str] = {}
        
        browser_config = BrowserConfig(
            headless=True,
            verbose=False  # –û—Ç–∫–ª—é—á–∞–µ–º verbose —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å Unicode –≤ Windows
        )
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–∞–π—Ç–æ–≤
        is_yuque = 'yuque.com' in base_url
        
        if is_yuque:
            # Yuque —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            crawl_config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                delay_before_return_html=6.0,  # –ö–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É —Å–∫–æ—Ä–æ—Å—Ç—å—é –∏ –ø–æ–ª–Ω–æ—Ç–æ–π
                page_timeout=90000,  # 90 —Å–µ–∫—É–Ω–¥
                # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–∞–≤–∏–≥–∞—Ü–∏—é –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                excluded_tags=['script', 'style', 'noscript', 'iframe'],
                # –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º css_selector - –æ–Ω –±–ª–æ–∫–∏—Ä—É–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫
                # –£–¥–∞–ª—è–µ–º –æ–≤–µ—Ä–ª–µ–∏ –∏ –ø–æ–ø–∞–ø—ã
                remove_overlay_elements=True,
                # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                word_count_threshold=5,
            )
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            crawl_config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                excluded_tags=['script', 'style', 'nav', 'footer', 'aside', 'header'],
            )
        
        base_domain = urlparse(base_url).netloc
        
        crawler = AsyncWebCrawler(config=browser_config)
        
        try:
            await crawler.__aenter__()
            
            while to_visit and len(visited) < max_pages:
                current_url, depth = to_visit.pop(0)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –ø–æ—Å–µ—â—ë–Ω–Ω—ã–µ
                if current_url in visited:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–ª—É–±–∏–Ω—É
                if depth > max_depth:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–µ–Ω
                if urlparse(current_url).netloc != base_domain:
                    continue
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã (–Ω–µ HTML)
                if any(current_url.endswith(ext) for ext in ['.pdf', '.zip', '.jpg', '.png', '.gif', '.svg']):
                    continue
                
                visited.add(current_url)
                logger.info(f"[{len(visited)}/{max_pages}] Depth {depth}: {current_url}")
                
                try:
                    result = await crawler.arun(url=current_url, config=crawl_config)
                    
                    if result.success and result.markdown:
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º markdown
                        results[current_url] = result.markdown
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏
                        internal_links = result.links.get('internal', [])
                        logger.info(f"Found {len(internal_links)} internal links")
                        
                        for link_obj in internal_links:
                            link_url = link_obj.get('href', '')
                            if link_url and link_url not in visited:
                                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL
                                full_url = urljoin(current_url, link_url)
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ç–æ—Ç –∂–µ –¥–æ–º–µ–Ω
                                if urlparse(full_url).netloc == base_domain:
                                    # –î–ª—è yuque –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ —Ç–æ—Ç –∂–µ —Ä–∞–∑–¥–µ–ª –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
                                    if is_yuque:
                                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—ã–π –ø—É—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä /ndx6g9/0.8.en)
                                        base_path = '/'.join(urlparse(base_url).path.split('/')[:3])
                                        current_path = urlparse(full_url).path
                                        if current_path.startswith(base_path):
                                            logger.debug(f"Adding to queue: {full_url}")
                                            to_visit.append((full_url, depth + 1))
                                        else:
                                            logger.debug(f"Skipping (wrong base path): {full_url}")
                                    else:
                                        to_visit.append((full_url, depth + 1))
                                else:
                                    logger.debug(f"Skipping (wrong domain): {full_url}")
                    else:
                        logger.warning(f"Failed to crawl {current_url}: {result.error_message}")
                        
                except Exception as e:
                    logger.error(f"Error crawling {current_url}: {str(e)}")
                    # –ü—Ä–∏ –æ—à–∏–±–∫–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
                    continue
                
                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                await asyncio.sleep(0.5)
        
        finally:
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ –∫—Ä–∞—É–ª–µ—Ä–∞
            try:
                await crawler.__aexit__(None, None, None)
            except Exception as e:
                logger.warning(f"Error closing crawler: {e}")
        
        return results
    
    def _save_markdown(self, url: str, content: str, output_dir: Path, metadata: dict = None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ markdown —Ñ–∞–π–ª–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL
        parsed = urlparse(url)
        path_parts = [p for p in parsed.path.split('/') if p]
        
        if not path_parts:
            filename = 'index.md'
        else:
            filename = '-'.join(path_parts) + '.md'
        
        # –£–±–∏—Ä–∞–µ–º –Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        filename = re.sub(r'[^\w\-.]', '_', filename)
        
        filepath = output_dir / filename
        
        # –î–æ–±–∞–≤–ª—è–µ–º YAML front matter
        yaml_front_matter = {
            'title': metadata.get('title', filename.replace('.md', '').replace('-', ' ').title()),
            'source': url,
            'crawled_at': datetime.now().isoformat(),
            'file_hash': hashlib.md5(content.encode()).hexdigest(),
            'word_count': len(content.split()),
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('---\n')
            yaml.dump(yaml_front_matter, f, allow_unicode=True)
            f.write('---\n\n')
            f.write(content)
        
        return filepath
    
    async def crawl_app(self, app_id: str):
        """–ö—Ä–∞—É–ª–∏–Ω–≥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
        apps = self._get_enabled_apps()
        app = next((a for a in apps if a['id'] == app_id), None)
        
        if not app:
            logger.error(f"App '{app_id}' not found or not enabled")
            return
        
        logger.info(f"üöÄ Starting crawl for: {app.get('name', app_id)}")
        logger.info(f"   URL: {app['url']}")
        
        output_dir = self.base_output_dir / app_id
        output_dir.mkdir(parents=True, exist_ok=True)
        
        max_depth = app.get('depth', 2)
        max_pages = app.get('max_pages', 100)
        
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –∫—Ä–∞—É–ª–∏–Ω–≥
        results = await self.crawl_recursive(
            start_url=app['url'],
            base_url=app['url'],
            output_dir=output_dir,
            max_depth=max_depth,
            max_pages=max_pages
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        logger.info(f"üíæ Saving {len(results)} pages...")
        for url, markdown in results.items():
            self._save_markdown(url, markdown, output_dir, {'title': url.split('/')[-1]})
        
        logger.success(f"‚úÖ Crawl completed: {len(results)} pages saved to {output_dir}")


@click.command()
@click.option('--app', required=True, help='App ID to crawl (from sources.yaml)')
@click.option('--config', default='config/sources.yaml', help='Path to config file')
def main(app: str, config: str):
    """DocAgent Crawler using Crawl4AI"""
    crawler = DocAgentCrawl4AI(config_path=config)
    asyncio.run(crawler.crawl_app(app))


if __name__ == "__main__":
    main()
