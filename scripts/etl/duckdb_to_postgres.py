#!/usr/bin/env python3
"""
ETL —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ DuckDB –≤ PostgreSQL.

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    DuckDB (osv_source.duckdb) ‚Üí PostgreSQL (analytics)
    
    DuckDB schemas:
        - raw: –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Å—á–µ—Ç–∞–º (osv_51, osv_60, etc.)
        - consolidated: –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (osv_detailed, osv_summary)
    
    PostgreSQL schemas:
        - history: —Å—ã—Ä—ã–µ OSV –¥–∞–Ω–Ω—ã–µ
        - analytics: –∞–≥—Ä–µ–≥–∞—Ç—ã + AI —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        - dds: —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ –∏ –º–∞—Å—Ç–µ—Ä-–¥–∞–Ω–Ω—ã–µ

Usage:
    python3 duckdb_to_postgres.py
    python3 duckdb_to_postgres.py --full  # –ø–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞
    python3 duckdb_to_postgres.py --incremental  # —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏
"""

import duckdb
import psycopg2
from psycopg2.extras import execute_batch
import os
from datetime import datetime
import argparse
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
DUCKDB_PATH = '/opt/docagent/data/duckdb/osv_source.duckdb'
PG_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'analytics',
    'user': 'analytics_user',
    'password': 'analytics_secure_2025'
}

class DuckDBToPostgres:
    """ETL –ø—Ä–æ—Ü–µ—Å—Å DuckDB ‚Üí PostgreSQL"""
    
    def __init__(self, duck_path: str, pg_config: dict):
        self.duck_path = duck_path
        self.pg_config = pg_config
        self.duck_con = None
        self.pg_con = None
        
    def connect(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –æ–±–µ–∏–º –±–∞–∑–∞–º"""
        logger.info("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ DuckDB...")
        self.duck_con = duckdb.connect(self.duck_path, read_only=True)
        
        logger.info("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL...")
        self.pg_con = psycopg2.connect(**self.pg_config)
        self.pg_con.autocommit = False
        
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        if self.duck_con:
            self.duck_con.close()
        if self.pg_con:
            self.pg_con.close()
            
    def create_postgres_schemas(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ö–µ–º –≤ PostgreSQL"""
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ö–µ–º PostgreSQL...")
        
        with self.pg_con.cursor() as cur:
            # –°–æ–∑–¥–∞–µ–º —Å—Ö–µ–º—ã
            cur.execute("CREATE SCHEMA IF NOT EXISTS history")
            cur.execute("CREATE SCHEMA IF NOT EXISTS analytics") 
            
            # –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
            cur.execute("""
                COMMENT ON SCHEMA history IS 
                '–°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ OSV –∏–∑ DuckDB (–¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Å—á–µ—Ç–∞–º)'
            """)
            cur.execute("""
                COMMENT ON SCHEMA analytics IS 
                '–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ + —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã AI –∞–≥–µ–Ω—Ç–æ–≤'
            """)
            
        self.pg_con.commit()
        logger.info("‚úÖ –°—Ö–µ–º—ã —Å–æ–∑–¥–∞–Ω—ã")
        
    def load_consolidated_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö OSV (DuckDB consolidated ‚Üí PG history)"""
        logger.info("\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ consolidated.osv_detailed –≤ history...")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –≤ PostgreSQL
        with self.pg_con.cursor() as cur:
            cur.execute("""
                DROP TABLE IF EXISTS history.osv_detailed CASCADE;
                CREATE TABLE history.osv_detailed (
                    id SERIAL PRIMARY KEY,
                    company_name VARCHAR(255),
                    inn VARCHAR(20),
                    period VARCHAR(20),
                    account VARCHAR(10),
                    subkonto TEXT,
                    opening_debit NUMERIC(18,2),
                    opening_credit NUMERIC(18,2),
                    turnover_debit NUMERIC(18,2),
                    turnover_credit NUMERIC(18,2),
                    closing_debit NUMERIC(18,2),
                    closing_credit NUMERIC(18,2),
                    source_file VARCHAR(255),
                    import_date TIMESTAMP,
                    etl_loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
        self.pg_con.commit()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ DuckDB
        df = self.duck_con.execute("""
            SELECT 
                company_name, inn, period, account, subkonto,
                opening_debit, opening_credit, 
                turnover_debit, turnover_credit,
                closing_debit, closing_credit,
                source_file, import_date
            FROM consolidated.osv_detailed
        """).fetchdf()
        
        logger.info(f"  –ù–∞–π–¥–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –≤ DuckDB")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º datetime64 –≤ string –¥–ª—è PostgreSQL
        if 'import_date' in df.columns:
            df['import_date'] = df['import_date'].astype(str)
        
        # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ None –¥–ª—è PostgreSQL NULL
        df = df.where(df.notna(), None)
        
        # –í—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã–º–∏ –±–∞—Ç—á–∞–º–∏
        if len(df) > 0:
            with self.pg_con.cursor() as cur:
                records = [tuple(row) for row in df.values]
                execute_batch(cur, """
                    INSERT INTO history.osv_detailed (
                        company_name, inn, period, account, subkonto,
                        opening_debit, opening_credit, 
                        turnover_debit, turnover_credit,
                        closing_debit, closing_credit,
                        source_file, import_date
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, list(records))
                
            self.pg_con.commit()
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –≤ history.osv_detailed")
            
        # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è osv_h1_summary (H1 = first half-year data)
        logger.info("\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ consolidated.osv_summary –≤ history.osv_h1_summary...")
        
        with self.pg_con.cursor() as cur:
            cur.execute("""
                DROP TABLE IF EXISTS history.osv_h1_summary CASCADE;
                CREATE TABLE history.osv_h1_summary (
                    id SERIAL PRIMARY KEY,
                    company_name VARCHAR(255),
                    inn VARCHAR(20),
                    period VARCHAR(20),
                    account VARCHAR(10),
                    account_name TEXT,
                    opening_debit NUMERIC(18,2),
                    opening_credit NUMERIC(18,2),
                    turnover_debit NUMERIC(18,2),
                    turnover_credit NUMERIC(18,2),
                    closing_debit NUMERIC(18,2),
                    closing_credit NUMERIC(18,2),
                    source_file VARCHAR(255),
                    import_date TIMESTAMP,
                    etl_loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
        df_summary = self.duck_con.execute("""
            SELECT 
                company_name, inn, period, account, account_name,
                opening_debit, opening_credit, 
                turnover_debit, turnover_credit,
                closing_debit, closing_credit,
                source_file, import_date
            FROM consolidated.osv_summary
        """).fetchdf()
        
        logger.info(f"  –ù–∞–π–¥–µ–Ω–æ {len(df_summary)} –∑–∞–ø–∏—Å–µ–π –≤ DuckDB")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º datetime64 –≤ string
        if 'import_date' in df_summary.columns:
            df_summary['import_date'] = df_summary['import_date'].astype(str)
        
        # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ None
        df_summary = df_summary.where(df_summary.notna(), None)
        
        if len(df_summary) > 0:
            with self.pg_con.cursor() as cur:
                records = [tuple(row) for row in df_summary.values]
                execute_batch(cur, """
                    INSERT INTO history.osv_h1_summary (
                        company_name, inn, period, account, account_name,
                        opening_debit, opening_credit, 
                        turnover_debit, turnover_credit,
                        closing_debit, closing_credit,
                        source_file, import_date
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, list(records))
                
            self.pg_con.commit()
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df_summary)} –∑–∞–ø–∏—Å–µ–π –≤ history.osv_h1_summary")
            
    def load_raw_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (DuckDB raw ‚Üí PG history)"""
        logger.info("\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ raw.osv_51 (—Ä–∞—Å—á–µ—Ç–Ω—ã–µ —Å—á–µ—Ç–∞)...")
        
        with self.pg_con.cursor() as cur:
            cur.execute("""
                DROP TABLE IF EXISTS history.osv_51 CASCADE;
                CREATE TABLE history.osv_51 (
                    id SERIAL PRIMARY KEY,
                    filename VARCHAR(255),
                    company_raw VARCHAR(255),
                    period VARCHAR(20),
                    dds_item TEXT,
                    inflow NUMERIC(18,2),
                    outflow NUMERIC(18,2),
                    internal_move_dt NUMERIC(18,2),
                    internal_move_kt BIGINT,
                    etl_loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
        df = self.duck_con.execute("SELECT * FROM raw.osv_51").fetchdf()
        logger.info(f"  –ù–∞–π–¥–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
        
        if len(df) > 0:
            with self.pg_con.cursor() as cur:
                records = [tuple(row) for row in df.values]
                execute_batch(cur, """
                    INSERT INTO history.osv_51 (
                        filename, company_raw, period, dds_item, 
                        inflow, outflow, internal_move_dt, internal_move_kt
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                """, list(records))
                
            self.pg_con.commit()
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –≤ history.osv_51")
            
        # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å—á–µ—Ç–æ–≤ (60, 62, 91)
        for account in ['60', '62', '91']:
            logger.info(f"\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ raw.osv_{account}...")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü—ã –∏–∑ DuckDB
            df_account = self.duck_con.execute(f"SELECT * FROM raw.osv_{account} LIMIT 1").fetchdf()
            
            if len(df_account.columns) > 0:
                # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ DuckDB —Å—Ö–µ–º—ã
                columns_def = []
                for col in df_account.columns:
                    dtype = df_account[col].dtype
                    if dtype == 'object':
                        pg_type = 'TEXT'
                    elif dtype == 'float64':
                        pg_type = 'NUMERIC(18,2)'
                    elif dtype == 'int64':
                        pg_type = 'BIGINT'
                    else:
                        pg_type = 'TEXT'
                    columns_def.append(f"{col} {pg_type}")
                
                with self.pg_con.cursor() as cur:
                    cur.execute(f"""
                        DROP TABLE IF EXISTS history.osv_{account} CASCADE;
                        CREATE TABLE history.osv_{account} (
                            id SERIAL PRIMARY KEY,
                            {', '.join(columns_def)},
                            etl_loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                    
                df_full = self.duck_con.execute(f"SELECT * FROM raw.osv_{account}").fetchdf()
                logger.info(f"  –ù–∞–π–¥–µ–Ω–æ {len(df_full)} –∑–∞–ø–∏—Å–µ–π")
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ NaN –∏ datetime
                df_full = df_full.where(df_full.notna(), None)
                for col in df_full.columns:
                    if df_full[col].dtype == 'datetime64[ns]':
                        df_full[col] = df_full[col].astype(str)
                
                if len(df_full) > 0:
                    records = [tuple(row) for row in df_full.values]
                    placeholders = ', '.join(['%s'] * len(df_full.columns))
                    col_names = ', '.join(df_full.columns)
                    
                    with self.pg_con.cursor() as cur:
                        execute_batch(cur, f"""
                            INSERT INTO history.osv_{account} ({col_names})
                            VALUES ({placeholders})
                        """, list(records))
                        
                    self.pg_con.commit()
                    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df_full)} –∑–∞–ø–∏—Å–µ–π –≤ history.osv_{account}")
                    
    def load_costs_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –ø–æ —Å—á–µ—Ç–∞–º 20, 26, 44 (DuckDB raw ‚Üí PG history)"""
        logger.info("\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ raw.osv_costs ‚Üí history.osv_9m_costs (—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–∞—Ç—Ä–∞—Ç –∑–∞ 9 –º–µ—Å—è—Ü–µ–≤)...")
        
        with self.pg_con.cursor() as cur:
            cur.execute("""
                DROP TABLE IF EXISTS history.osv_9m_costs CASCADE;
                CREATE TABLE history.osv_9m_costs (
                    id SERIAL PRIMARY KEY,
                    filename VARCHAR(255),
                    company_raw VARCHAR(255),
                    period VARCHAR(50),
                    account_type VARCHAR(10),
                    cost_item TEXT,
                    amount_dt NUMERIC(18,2),
                    amount_kt NUMERIC(18,2),
                    etl_loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
        df = self.duck_con.execute("SELECT * FROM raw.osv_costs").fetchdf()
        logger.info(f"  –ù–∞–π–¥–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
        
        if len(df) > 0:
            # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ None
            df = df.where(df.notna(), None)
            
            with self.pg_con.cursor() as cur:
                records = [tuple(row) for row in df.values]
                execute_batch(cur, """
                    INSERT INTO history.osv_9m_costs (
                        filename, company_raw, period, account_type, 
                        cost_item, amount_dt, amount_kt
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s)
                """, list(records))
                
            self.pg_con.commit()
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –≤ history.osv_9m_costs")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—á–µ—Ç–∞–º
            stats = self.duck_con.execute("""
                SELECT account_type, COUNT(*) as cnt 
                FROM raw.osv_costs 
                GROUP BY account_type 
                ORDER BY account_type
            """).fetchdf()
            for _, row in stats.iterrows():
                logger.info(f"  –°—á–µ—Ç {row['account_type']}: {row['cnt']} –∑–∞–ø–∏—Å–µ–π")
    
    def load_reference_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤ (DuckDB raw ‚Üí —É–∂–µ –µ—Å—Ç—å –≤ master —Å—Ö–µ–º–µ PostgreSQL)"""
        logger.info("\nüì¶ –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –≤ —Å—Ö–µ–º–µ master")
        logger.info("  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ")
        # –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ companies —É–∂–µ –≤ master.companies
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è
            
    def create_analytics_views(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π"""
        logger.info("\nüìä –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π...")
        
        with self.pg_con.cursor() as cur:
            # View: –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –ø–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º (–∏–∑ history.osv_detailed)
            cur.execute("""
                CREATE OR REPLACE VIEW analytics.v_consolidated_by_org AS
                SELECT 
                    company_name,
                    inn,
                    period,
                    SUM(turnover_debit) as total_debit,
                    SUM(turnover_credit) as total_credit,
                    SUM(closing_debit - opening_debit) as debit_change,
                    SUM(closing_credit - opening_credit) as credit_change
                FROM history.osv_detailed
                GROUP BY company_name, inn, period
                ORDER BY company_name, period
            """)
            
            # View: –°–≤–æ–¥–∫–∞ –∑–∞—Ç—Ä–∞—Ç –ø–æ –∫–æ–º–ø–∞–Ω–∏—è–º (–∏–∑ history.osv_9m_costs)
            cur.execute("""
                CREATE OR REPLACE VIEW analytics.v_costs_by_company AS
                SELECT 
                    company_raw as company_name,
                    account_type,
                    period,
                    COUNT(DISTINCT cost_item) as items_count,
                    SUM(amount_dt) as total_costs_dt,
                    SUM(amount_kt) as total_costs_kt,
                    SUM(amount_dt - amount_kt) as net_costs
                FROM history.osv_9m_costs
                GROUP BY company_raw, account_type, period
                ORDER BY company_raw, account_type
            """)
            
        self.pg_con.commit()
        logger.info("‚úÖ –ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–æ–∑–¥–∞–Ω—ã")
        
    def run_full_etl(self):
        """–ü–æ–ª–Ω—ã–π ETL –ø—Ä–æ—Ü–µ—Å—Å"""
        try:
            self.connect()
            
            logger.info("="*60)
            logger.info("–°–¢–ê–†–¢ ETL: DuckDB ‚Üí PostgreSQL")
            logger.info("="*60)
            
            self.create_postgres_schemas()
            self.load_reference_data()
            self.load_consolidated_data()
            self.load_raw_data()
            self.load_costs_data()
            self.create_analytics_views()
            
            logger.info("\n" + "="*60)
            logger.info("‚úÖ ETL –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û")
            logger.info("="*60)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            with self.pg_con.cursor() as cur:
                cur.execute("""
                    SELECT 
                        schemaname, 
                        relname as tablename, 
                        n_live_tup as row_count
                    FROM pg_stat_user_tables
                    WHERE schemaname IN ('history', 'analytics', 'master')
                    ORDER BY schemaname, relname
                """)
                
                logger.info("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
                for row in cur.fetchall():
                    logger.info(f"  {row[0]}.{row[1]}: {row[2]:,} –∑–∞–ø–∏—Å–µ–π")
                    
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ ETL: {e}")
            if self.pg_con:
                self.pg_con.rollback()
            raise
        finally:
            self.close()


def main():
    parser = argparse.ArgumentParser(description='ETL: DuckDB ‚Üí PostgreSQL')
    parser.add_argument('--full', action='store_true', help='–ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞')
    parser.add_argument('--incremental', action='store_true', help='–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞')
    args = parser.parse_args()
    
    etl = DuckDBToPostgres(DUCKDB_PATH, PG_CONFIG)
    
    if args.incremental:
        logger.info("–†–µ–∂–∏–º: –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (TODO)")
        # TODO: —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É
    else:
        etl.run_full_etl()


if __name__ == '__main__':
    main()
