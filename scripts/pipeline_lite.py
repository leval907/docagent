#!/usr/bin/env python3
"""
–õ–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å chunking + embedding + –ø–æ–∏—Å–∫
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ SQLite –∏ sentence-transformers, –±–µ–∑ Docker
"""

import sqlite3
import hashlib
import json
import asyncio
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple
import re

# –î–ª—è embeddings
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
except ImportError:
    print("‚ö†Ô∏è  –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sentence-transformers")
    exit(1)

# –î–ª—è –∫—Ä–∞—É–ª–∏–Ω–≥–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã)
try:
    from crawl4ai import AsyncWebCrawler
except ImportError:
    print("‚ö†Ô∏è  Crawl4AI –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ñ–∞–π–ª–∞–º–∏")
    AsyncWebCrawler = None


class DocumentPipelineLite:
    """–õ–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(
        self,
        db_path: str = "docagent_lite.db",
        model_name: str = "all-MiniLM-L6-v2",  # –õ–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å, 384 dim
        chunk_size: int = 500,  # –°–ª–æ–≤ –Ω–∞ chunk
        chunk_overlap: int = 50  # –°–ª–æ–≤ overlap
    ):
        self.db_path = Path(db_path)
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
        self.init_database()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è embeddings
        print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
        self.model = SentenceTransformer(model_name)
        self.embedding_dim = self.model.get_sentence_embedding_dimension()
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.embedding_dim}")
    
    def init_database(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –≤ SQLite"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url TEXT UNIQUE NOT NULL,
                title TEXT,
                app_id TEXT,
                content TEXT,
                word_count INTEGER,
                chunk_count INTEGER DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # –¢–∞–±–ª–∏—Ü–∞ chunks —Å embeddings
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS chunks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                document_id INTEGER NOT NULL,
                chunk_index INTEGER NOT NULL,
                chunk_text TEXT NOT NULL,
                word_count INTEGER,
                embedding BLOB,  -- –•—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∫ pickle/json
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (document_id) REFERENCES documents(id) ON DELETE CASCADE
            )
        """)
        
        # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_documents_app 
            ON documents(app_id)
        """)
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_chunks_document 
            ON chunks(document_id)
        """)
        
        conn.commit()
        conn.close()
        print(f"‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: {self.db_path}")
    
    def clean_markdown(self, content: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ markdown –æ—Ç UI —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        patterns = [
            (r'\[‰∏ã‰∏ÄÈ°µ\]\([^)]+\)', ''),  # –ù–∞–≤–∏–≥–∞—Ü–∏—è
            (r'!\[.*?\]\(.*?\)', ''),  # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            (r'\[.*?\]\(javascript:.*?\)', ''),  # JS —Å—Å—ã–ª–∫–∏
            (r'#+\s*ÁõÆÂΩï.*?(?=\n##|\Z)', '', re.DOTALL),  # –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ
            (r'\n{3,}', '\n\n'),  # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã
        ]
        
        for pattern, replacement, *flags in patterns:
            flag = flags[0] if flags else 0
            content = re.sub(pattern, replacement, content, flags=flag)
        
        return content.strip()
    
    def chunk_text(self, text: str) -> List[Dict[str, any]]:
        """–†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ chunks —Å overlap"""
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), self.chunk_size - self.chunk_overlap):
            chunk_words = words[i:i + self.chunk_size]
            chunk_text = ' '.join(chunk_words)
            
            if len(chunk_words) < 50:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
                continue
            
            chunks.append({
                'index': len(chunks),
                'text': chunk_text,
                'word_count': len(chunk_words)
            })
        
        return chunks
    
    def generate_embeddings(self, texts: List[str]) -> np.ndarray:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        return self.model.encode(texts, show_progress_bar=True)
    
    def save_document(
        self,
        url: str,
        title: str,
        content: str,
        app_id: str = "default"
    ) -> int:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å chunking –∏ embeddings"""
        
        # –û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        clean_content = self.clean_markdown(content)
        word_count = len(clean_content.split())
        
        # Chunking
        chunks = self.chunk_text(clean_content)
        print(f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(chunks)} chunks")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings
        chunk_texts = [c['text'] for c in chunks]
        embeddings = self.generate_embeddings(chunk_texts)
        print(f"üß† Embeddings —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã: {embeddings.shape}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            cursor.execute("""
                INSERT OR REPLACE INTO documents 
                (url, title, app_id, content, word_count, chunk_count, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            """, (url, title, app_id, clean_content, word_count, len(chunks)))
            
            doc_id = cursor.lastrowid
            
            # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö chunks –µ—Å–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
            cursor.execute("DELETE FROM chunks WHERE document_id = ?", (doc_id,))
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ chunks —Å embeddings
            for chunk, embedding in zip(chunks, embeddings):
                # –°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è embedding –∫–∞–∫ JSON
                embedding_json = json.dumps(embedding.tolist())
                
                cursor.execute("""
                    INSERT INTO chunks 
                    (document_id, chunk_index, chunk_text, word_count, embedding)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    doc_id,
                    chunk['index'],
                    chunk['text'],
                    chunk['word_count'],
                    embedding_json
                ))
            
            conn.commit()
            print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: ID={doc_id}, chunks={len(chunks)}")
            return doc_id
            
        finally:
            conn.close()
    
    def cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    def search(
        self,
        query: str,
        app_id: Optional[str] = None,
        limit: int = 10,
        min_similarity: float = 0.3
    ) -> List[Dict]:
        """–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ chunks"""
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.model.encode([query])[0]
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö chunks (–º–æ–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ app_id)
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        if app_id:
            cursor.execute("""
                SELECT c.id, c.document_id, c.chunk_index, c.chunk_text, 
                       c.embedding, d.title, d.url, d.app_id
                FROM chunks c
                JOIN documents d ON c.document_id = d.id
                WHERE d.app_id = ?
            """, (app_id,))
        else:
            cursor.execute("""
                SELECT c.id, c.document_id, c.chunk_index, c.chunk_text, 
                       c.embedding, d.title, d.url, d.app_id
                FROM chunks c
                JOIN documents d ON c.document_id = d.id
            """)
        
        results = []
        for row in cursor.fetchall():
            chunk_id, doc_id, chunk_idx, chunk_text, embedding_json, title, url, app = row
            
            # –î–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è embedding
            chunk_embedding = np.array(json.loads(embedding_json))
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ similarity
            similarity = self.cosine_similarity(query_embedding, chunk_embedding)
            
            if similarity >= min_similarity:
                results.append({
                    'chunk_id': chunk_id,
                    'document_id': doc_id,
                    'chunk_index': chunk_idx,
                    'title': title,
                    'url': url,
                    'app_id': app,
                    'chunk_text': chunk_text,
                    'similarity': float(similarity)
                })
        
        conn.close()
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ similarity
        results.sort(key=lambda x: x['similarity'], reverse=True)
        
        return results[:limit]
    
    async def crawl_and_process(
        self,
        url: str,
        app_id: str = "default"
    ):
        """–ö—Ä–∞—É–ª–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞"""
        if AsyncWebCrawler is None:
            print("‚ùå Crawl4AI –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            return
        
        print(f"üï∑Ô∏è  –ö—Ä–∞—É–ª–∏–Ω–≥: {url}")
        
        async with AsyncWebCrawler(verbose=False) as crawler:
            result = await crawler.arun(
                url=url,
                bypass_cache=True,
                wait_for="networkidle",
                delay_before_return_html=3.0
            )
            
            if result.success:
                self.save_document(
                    url=url,
                    title=result.title or "Untitled",
                    content=result.markdown,
                    app_id=app_id
                )
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∫—Ä–∞—É–ª–∏–Ω–≥–∞: {result.error_message}")
    
    def load_from_file(
        self,
        file_path: str,
        url: str = None,
        title: str = None,
        app_id: str = "default"
    ):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
        path = Path(file_path)
        
        if not path.exists():
            print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            return
        
        content = path.read_text(encoding='utf-8')
        
        self.save_document(
            url=url or f"file://{path.absolute()}",
            title=title or path.stem,
            content=content,
            app_id=app_id
        )
    
    def stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –±–∞–∑–µ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º
        cursor.execute("""
            SELECT app_id, COUNT(*) as docs, SUM(word_count) as words, SUM(chunk_count) as chunks
            FROM documents
            GROUP BY app_id
        """)
        
        apps = []
        for row in cursor.fetchall():
            apps.append({
                'app_id': row[0],
                'documents': row[1],
                'words': row[2],
                'chunks': row[3]
            })
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        cursor.execute("SELECT COUNT(*) FROM documents")
        total_docs = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM chunks")
        total_chunks = cursor.fetchone()[0]
        
        conn.close()
        
        return {
            'total_documents': total_docs,
            'total_chunks': total_chunks,
            'apps': apps
        }


def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description="–õ–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤")
    parser.add_argument("--db", default="docagent_lite.db", help="–ü—É—Ç—å –∫ SQLite –ë–î")
    parser.add_argument("--model", default="all-MiniLM-L6-v2", help="–ú–æ–¥–µ–ª—å –¥–ª—è embeddings")
    
    subparsers = parser.add_subparsers(dest='command', help='–ö–æ–º–∞–Ω–¥—ã')
    
    # –ö–æ–º–∞–Ω–¥–∞: load
    load_parser = subparsers.add_parser('load', help='–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞')
    load_parser.add_argument("file", help="–ü—É—Ç—å –∫ markdown —Ñ–∞–π–ª—É")
    load_parser.add_argument("--app", default="default", help="ID –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è")
    load_parser.add_argument("--title", help="–ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    
    # –ö–æ–º–∞–Ω–¥–∞: crawl
    crawl_parser = subparsers.add_parser('crawl', help='–ö—Ä–∞—É–ª–∏–Ω–≥ URL')
    crawl_parser.add_argument("url", help="URL –¥–ª—è –∫—Ä–∞—É–ª–∏–Ω–≥–∞")
    crawl_parser.add_argument("--app", default="default", help="ID –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è")
    
    # –ö–æ–º–∞–Ω–¥–∞: search
    search_parser = subparsers.add_parser('search', help='–ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É')
    search_parser.add_argument("query", help="–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å")
    search_parser.add_argument("--app", help="–§–∏–ª—å—Ç—Ä –ø–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—é")
    search_parser.add_argument("--limit", type=int, default=5, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    
    # –ö–æ–º–∞–Ω–¥–∞: stats
    stats_parser = subparsers.add_parser('stats', help='–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ë–î')
    
    args = parser.parse_args()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è pipeline
    pipeline = DocumentPipelineLite(
        db_path=args.db,
        model_name=args.model
    )
    
    if args.command == 'load':
        pipeline.load_from_file(
            file_path=args.file,
            title=args.title,
            app_id=args.app
        )
        
    elif args.command == 'crawl':
        asyncio.run(pipeline.crawl_and_process(
            url=args.url,
            app_id=args.app
        ))
        
    elif args.command == 'search':
        results = pipeline.search(
            query=args.query,
            app_id=args.app,
            limit=args.limit
        )
        
        print(f"\nüîç –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}\n")
        for i, result in enumerate(results, 1):
            print(f"{i}. [{result['title']}] ({result['similarity']:.3f})")
            print(f"   URL: {result['url']}")
            print(f"   App: {result['app_id']}")
            print(f"   –¢–µ–∫—Å—Ç: {result['chunk_text'][:200]}...")
            print()
        
    elif args.command == 'stats':
        stats = pipeline.stats()
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
        print(f"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {stats['total_documents']}")
        print(f"Chunks: {stats['total_chunks']}")
        print(f"\n–ü–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º:")
        for app in stats['apps']:
            print(f"  {app['app_id']}: {app['documents']} docs, {app['words']} words, {app['chunks']} chunks")
    
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
