# üß™ –õ–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤

**SQLite + sentence-transformers** - –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å chunking, embeddings –∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –ª–æ–∫–∞–ª—å–Ω–æ.

## üéØ –ß—Ç–æ —ç—Ç–æ?

–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è **–±—ã—Å—Ç—Ä—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤** –Ω–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–µ:
- ‚úÖ **–ë–µ–∑ Docker** - –ø—Ä–æ—Å—Ç–æ Python
- ‚úÖ **SQLite** - –≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –ë–î, –æ–¥–∏–Ω —Ñ–∞–π–ª
- ‚úÖ **sentence-transformers** - –ª–æ–∫–∞–ª—å–Ω—ã–µ embeddings –±–µ–∑ API
- ‚úÖ **–õ–µ–≥–∫–æ–≤–µ—Å–Ω–æ** - –º–æ–¥–µ–ª—å ~80MB, –±—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
pip install sentence-transformers crawl4ai
```

### 2. –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤

```bash
# –ó–∞–≥—Ä—É–∑–∏—Ç—å markdown —Ñ–∞–π–ª
python scripts/pipeline_lite.py load knowledge_base/openspg/intro.md --app openspg --title "OpenSPG Introduction"

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏
for file in knowledge_base/openspg/*.md; do
    python scripts/pipeline_lite.py load "$file" --app openspg
done
```

### 3. –ü–æ–∏—Å–∫

```bash
# –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
python scripts/pipeline_lite.py search "knowledge graph"

# –ü–æ–∏—Å–∫ —Ç–æ–ª—å–∫–æ –≤ OpenSPG
python scripts/pipeline_lite.py search "schema definition" --app openspg --limit 10
```

### 4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

```bash
python scripts/pipeline_lite.py stats
```

### 5. –ö—Ä–∞—É–ª–∏–Ω–≥ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

```bash
python scripts/pipeline_lite.py crawl "https://openspg.yuque.com/ndx6g9/manual/intro" --app openspg
```

## üìä –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä–∏?

### 1. Chunking
```
–î–æ–∫—É–º–µ–Ω—Ç (5000 —Å–ª–æ–≤)
    ‚Üì
Chunk 1 (500 —Å–ª–æ–≤)
Chunk 2 (500 —Å–ª–æ–≤, overlap 50)
Chunk 3 (500 —Å–ª–æ–≤, overlap 50)
...
Chunk 10 (500 —Å–ª–æ–≤)
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `chunk_size=500` - —Å–ª–æ–≤ –Ω–∞ chunk
- `chunk_overlap=50` - —Å–ª–æ–≤ overlap –º–µ–∂–¥—É chunks

### 2. Embeddings

–ú–æ–¥–µ–ª—å: `all-MiniLM-L6-v2`
- –†–∞–∑–º–µ—Ä: ~80MB
- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: 384
- –°–∫–æ—Ä–æ—Å—Ç—å: ~1000 chunks/—Å–µ–∫ –Ω–∞ CPU

–î–ª—è –∫–∞–∂–¥–æ–≥–æ chunk –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –≤–µ–∫—Ç–æ—Ä [384 —á–∏—Å–µ–ª]:
```python
chunk_text = "OpenSPG is a knowledge graph engine..."
embedding = model.encode(chunk_text)
# [0.123, -0.456, 0.789, ..., 0.234]  # 384 —á–∏—Å–ª–∞
```

### 3. –•—Ä–∞–Ω–µ–Ω–∏–µ –≤ SQLite

**–¢–∞–±–ª–∏—Ü–∞ `documents`:**
```sql
CREATE TABLE documents (
    id INTEGER PRIMARY KEY,
    url TEXT,
    title TEXT,
    app_id TEXT,
    content TEXT,
    word_count INTEGER,
    chunk_count INTEGER
);
```

**–¢–∞–±–ª–∏—Ü–∞ `chunks`:**
```sql
CREATE TABLE chunks (
    id INTEGER PRIMARY KEY,
    document_id INTEGER,
    chunk_index INTEGER,
    chunk_text TEXT,
    embedding BLOB  -- JSON —Å –≤–µ–∫—Ç–æ—Ä–æ–º [384 —á–∏—Å–ª–∞]
);
```

### 4. –ü–æ–∏—Å–∫

1. –ó–∞–ø—Ä–æ—Å ‚Üí embedding: `"knowledge graph"` ‚Üí `[0.1, -0.2, ...]`
2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–æ –≤—Å–µ–º–∏ chunks —á–µ—Ä–µ–∑ **cosine similarity**:
   ```python
   similarity = dot(query_vec, chunk_vec) / (norm(query_vec) * norm(chunk_vec))
   ```
3. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ similarity (0.0 - 1.0)
4. –í–æ–∑–≤—Ä–∞—Ç —Ç–æ–ø-N —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

## üí° –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ü—Ä–∏–º–µ—Ä 1: –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö OpenSPG –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

```bash
cd D:\docs\DocAgent

# Windows PowerShell
Get-ChildItem -Path knowledge_base\openspg\*.md | ForEach-Object {
    python scripts/pipeline_lite.py load $_.FullName --app openspg
}

# Linux/Mac
for file in knowledge_base/openspg/*.md; do
    python scripts/pipeline_lite.py load "$file" --app openspg
done
```

### –ü—Ä–∏–º–µ—Ä 2: –ü–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π

```python
from scripts.pipeline_lite import DocumentPipelineLite

pipeline = DocumentPipelineLite(db_path="docagent_lite.db")

# –ü–æ–∏—Å–∫
results = pipeline.search(
    query="How to build knowledge graph?",
    app_id="openspg",
    limit=5,
    min_similarity=0.5  # –¢–æ–ª—å–∫–æ –≤—ã—Å–æ–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
)

for result in results:
    print(f"{result['title']}: {result['similarity']:.3f}")
    print(f"  {result['chunk_text'][:150]}...")
```

### –ü—Ä–∏–º–µ—Ä 3: –ü—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ

```python
from scripts.pipeline_lite import DocumentPipelineLite

pipeline = DocumentPipelineLite()

# –î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç
pipeline.save_document(
    url="https://example.com/doc1",
    title="My Document",
    content="Long markdown content here...",
    app_id="myapp"
)

# –ü–æ–∏—Å–∫
results = pipeline.search("relevant query")
```

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞

### –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ embeddings

```bash
# –õ–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
python scripts/pipeline_lite.py --model all-MiniLM-L6-v2 load file.md

# –ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å (–±–æ–ª—å—à–µ —Ä–∞–∑–º–µ—Ä)
python scripts/pipeline_lite.py --model all-mpnet-base-v2 load file.md

# –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å
python scripts/pipeline_lite.py --model paraphrase-multilingual-MiniLM-L12-v2 load file.md
```

**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:**

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä | Dim | –°–∫–æ—Ä–æ—Å—Ç—å | –ö–∞—á–µ—Å—Ç–≤–æ |
|--------|--------|-----|----------|----------|
| all-MiniLM-L6-v2 | 80MB | 384 | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê |
| all-mpnet-base-v2 | 420MB | 768 | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê |
| paraphrase-multilingual-MiniLM-L12-v2 | 470MB | 384 | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê |

### –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ chunks

–ò–∑–º–µ–Ω–∏—Ç–µ –≤ –∫–æ–¥–µ `pipeline_lite.py`:

```python
pipeline = DocumentPipelineLite(
    chunk_size=1000,  # –ë–æ–ª—å—à–µ chunks = –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    chunk_overlap=100  # –ë–æ–ª—å—à–µ overlap = –º–µ–Ω—å—à–µ –ø–æ—Ç–µ—Ä—å
)
```

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
- –ö–æ—Ä–æ—Ç–∫–∏–µ chunks (300-500) - –ª—É—á—à–µ —Ç–æ—á–Ω–æ—Å—Ç—å
- –î–ª–∏–Ω–Ω—ã–µ chunks (1000-2000) - –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- Overlap 10-20% –æ—Ç chunk_size

## üìà –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### –¢–µ—Å—Ç—ã –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–µ

**Hardware**: Intel i5, 16GB RAM, –±–µ–∑ GPU

| –û–ø–µ—Ä–∞—Ü–∏—è | –°–∫–æ—Ä–æ—Å—Ç—å |
|----------|----------|
| –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ | ~3 —Å–µ–∫ |
| Chunking 10K —Å–ª–æ–≤ | ~0.1 —Å–µ–∫ |
| Embeddings 20 chunks | ~0.5 —Å–µ–∫ |
| –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ SQLite | ~0.1 —Å–µ–∫ |
| –ü–æ–∏—Å–∫ –ø–æ 1000 chunks | ~2 —Å–µ–∫ |

**–ü—Ä–∏–º–µ—Ä**: –î–æ–∫—É–º–µ–Ω—Ç OpenSPG (24 —Å—Ç—Ä–∞–Ω–∏—Ü—ã, 16K —Å–ª–æ–≤)
- Chunking: 32 chunks
- Embeddings: ~1.5 —Å–µ–∫—É–Ω–¥
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: ~0.2 —Å–µ–∫—É–Ω–¥—ã
- **–ò—Ç–æ–≥–æ: ~2 —Å–µ–∫—É–Ω–¥—ã –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç**

## üîç –ö–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞

### –ü—Ä–∏–º–µ—Ä —Ä–µ–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

**–ó–∞–ø—Ä–æ—Å**: `"How to define schema in OpenSPG?"`

```
1. [OpenSPG Schema Guide] (0.847)
   Schema definition in OpenSPG allows you to model domain knowledge using SPO triples...

2. [Quick Start Tutorial] (0.782)
   To get started with schemas, first define your entity types and their properties...

3. [Advanced Concepts] (0.691)
   Schema evolution and versioning are supported through migration scripts...
```

**Similarity score –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:**
- `0.9-1.0` - –ü–æ—á—Ç–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–π —Ç–µ–∫—Å—Ç
- `0.7-0.9` - –í—ã—Å–æ–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
- `0.5-0.7` - –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
- `0.3-0.5` - –ù–∏–∑–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
- `<0.3` - –ù–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ (—Ñ–∏–ª—å—Ç—Ä—É–µ—Ç—Å—è)

## üß™ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã

### –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 1: –†–∞–∑–º–µ—Ä chunks

```bash
# –¢–µ—Å—Ç 1: –ú–∞–ª–µ–Ω—å–∫–∏–µ chunks (300 —Å–ª–æ–≤)
python -c "
from scripts.pipeline_lite import DocumentPipelineLite
p = DocumentPipelineLite(chunk_size=300, chunk_overlap=30)
p.load_from_file('knowledge_base/openspg/intro.md', app_id='test1')
results = p.search('knowledge graph', app_id='test1')
print(f'Results: {len(results)}, Avg similarity: {sum(r[\"similarity\"] for r in results)/len(results):.3f}')
"

# –¢–µ—Å—Ç 2: –ë–æ–ª—å—à–∏–µ chunks (1000 —Å–ª–æ–≤)
python -c "
from scripts.pipeline_lite import DocumentPipelineLite
p = DocumentPipelineLite(chunk_size=1000, chunk_overlap=100)
p.load_from_file('knowledge_base/openspg/intro.md', app_id='test2')
results = p.search('knowledge graph', app_id='test2')
print(f'Results: {len(results)}, Avg similarity: {sum(r[\"similarity\"] for r in results)/len(results):.3f}')
"
```

### –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 2: –†–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏

```bash
# all-MiniLM-L6-v2 (–ª–µ–≥–∫–∞—è)
python scripts/pipeline_lite.py --model all-MiniLM-L6-v2 --db test_mini.db \
  load knowledge_base/openspg/intro.md

python scripts/pipeline_lite.py --db test_mini.db search "schema definition"

# all-mpnet-base-v2 (—Ç–æ—á–Ω–∞—è)
python scripts/pipeline_lite.py --model all-mpnet-base-v2 --db test_mpnet.db \
  load knowledge_base/openspg/intro.md

python scripts/pipeline_lite.py --db test_mpnet.db search "schema definition"
```

### –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 3: Multilingual

```bash
# –ê–Ω–≥–ª–∏–π—Å–∫–∏–π + –ö–∏—Ç–∞–π—Å–∫–∏–π
python scripts/pipeline_lite.py \
  --model paraphrase-multilingual-MiniLM-L12-v2 \
  load knowledge_base/openspg/intro.md

# –ü–æ–∏—Å–∫ –Ω–∞ —Ä—É—Å—Å–∫–æ–º
python scripts/pipeline_lite.py search "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ö–µ–º—ã –∑–Ω–∞–Ω–∏–π"
```

## üîÑ –ú–∏–≥—Ä–∞—Ü–∏—è –Ω–∞ production

–ö–æ–≥–¥–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∑–∞–∫–æ–Ω—á–µ–Ω—ã, –ª–µ–≥–∫–æ –º–∏–≥—Ä–∏—Ä–æ–≤–∞—Ç—å:

### SQLite ‚Üí PostgreSQL + pgvector

```python
# 1. –≠–∫—Å–ø–æ—Ä—Ç –∏–∑ SQLite
import sqlite3
import psycopg2

sqlite_conn = sqlite3.connect("docagent_lite.db")
pg_conn = psycopg2.connect("postgresql://...")

# 2. –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
for doc in sqlite_conn.execute("SELECT * FROM documents"):
    pg_conn.execute("INSERT INTO documents VALUES (...)")

# 3. Embeddings –≤ pgvector
for chunk in sqlite_conn.execute("SELECT * FROM chunks"):
    embedding = json.loads(chunk['embedding'])
    pg_conn.execute(
        "INSERT INTO chunks (embedding) VALUES (%s::vector)",
        (embedding,)
    )
```

### sentence-transformers ‚Üí OpenAI API

```python
# –ó–∞–º–µ–Ω–∞ –º–æ–¥–µ–ª–∏
# OLD:
model = SentenceTransformer("all-MiniLM-L6-v2")
embedding = model.encode(text)

# NEW:
import openai
response = openai.Embedding.create(
    model="text-embedding-ada-002",
    input=text
)
embedding = response['data'][0]['embedding']
```

## üêõ Troubleshooting

### –ü—Ä–æ–±–ª–µ–º–∞: –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è

```
OSError: Can't load tokenizer for 'all-MiniLM-L6-v2'
```

**–†–µ—à–µ–Ω–∏–µ**: –†—É—á–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
```bash
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"
```

### –ü—Ä–æ–±–ª–µ–º–∞: –ú–µ–¥–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫

–ï—Å–ª–∏ chunks > 10,000, –ø–æ–∏—Å–∫ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –º–µ–¥–ª–µ–Ω–Ω—ã–º.

**–†–µ—à–µ–Ω–∏—è:**
1. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ `app_id` –ø–µ—Ä–µ–¥ –ø–æ–∏—Å–∫–æ–º
2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ FAISS –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
3. –ú–∏–≥—Ä–∞—Ü–∏—è –Ω–∞ PostgreSQL + pgvector

### –ü—Ä–æ–±–ª–µ–º–∞: –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞

**–ü—Ä–∏—á–∏–Ω—ã:**
- –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ chunks (>1000 —Å–ª–æ–≤)
- –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ chunks (<200 —Å–ª–æ–≤)
- –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —è–∑—ã–∫–∞

**–†–µ—à–µ–Ω–∏—è:**
- –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: 400-600 —Å–ª–æ–≤
- Overlap: 10-15%
- Multilingual –º–æ–¥–µ–ª—å –¥–ª—è non-English

## üìö –î–∞–ª—å–Ω–µ–π—à–∏–µ —É–ª—É—á—à–µ–Ω–∏—è

### FAISS –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞

```python
import faiss

# –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
index = faiss.IndexFlatIP(384)  # Inner Product ~ Cosine
embeddings_matrix = np.array([...])  # –í—Å–µ embeddings
index.add(embeddings_matrix)

# –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫
query_vec = model.encode([query])[0]
D, I = index.search(query_vec.reshape(1, -1), k=10)
```

### –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (BM25 + Vector)

```python
from rank_bm25 import BM25Okapi

# BM25 –¥–ª—è keyword search
corpus = [chunk['text'].split() for chunk in chunks]
bm25 = BM25Okapi(corpus)

# –ö–æ–º–±–∏–Ω–∞—Ü–∏—è scores
keyword_scores = bm25.get_scores(query.split())
vector_scores = [cosine_sim(query_vec, chunk_vec) for chunk_vec in chunk_vecs]

# Weighted combination
final_scores = 0.3 * keyword_scores + 0.7 * vector_scores
```

### Reranking —Å cross-encoder

```python
from sentence_transformers import CrossEncoder

reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')

# –ü–µ—Ä–≤–∏—á–Ω—ã–π –ø–æ–∏—Å–∫ (top 100)
candidates = pipeline.search(query, limit=100)

# Reranking (top 10)
pairs = [(query, c['chunk_text']) for c in candidates]
scores = reranker.predict(pairs)
reranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)[:10]
```

## üìñ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è

- **sentence-transformers**: https://www.sbert.net/
- **–ú–æ–¥–µ–ª–∏**: https://www.sbert.net/docs/pretrained_models.html
- **Chunking —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏**: https://www.pinecone.io/learn/chunking-strategies/
- **Cosine similarity**: https://en.wikipedia.org/wiki/Cosine_similarity

---

**‚ú® –ì–æ—Ç–æ–≤ –∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º! –ó–∞–ø—É—Å—Ç–∏—Ç–µ `python scripts/pipeline_lite.py --help` –¥–ª—è –Ω–∞—á–∞–ª–∞.**
