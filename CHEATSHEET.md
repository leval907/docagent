# DocAgent - –®–ø–∞—Ä–≥–∞–ª–∫–∞ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–∞

## üöÄ –ë—ã—Å—Ç—Ä—ã–µ –∫–æ–º–∞–Ω–¥—ã

### –ê–∫—Ç–∏–≤–∞—Ü–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è
```bash
cd /opt/docagent
source venv/bin/activate
```

### –†–∞–±–æ—Ç–∞ —Å Excel/CSV –¥–∞–Ω–Ω—ã–º–∏ (DuckDB)

```python
from scripts.analytics.duckdb_analytics import DuckDBAnalytics

# –°–æ–∑–¥–∞—Ç—å –∞–Ω–∞–ª–∏—Ç–∏–∫—É
analytics = DuckDBAnalytics()

# –ò–º–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö
analytics.import_excel("sales_2024.xlsx")
analytics.import_csv("clients.csv")
analytics.import_json("products.json")

# –ê–≤—Ç–æ–ø–æ–∏—Å–∫ —Å–≤—è–∑–µ–π
relationships = analytics.analyze_relationships()

# SQL –∑–∞–ø—Ä–æ—Å
result = analytics.query("""
    SELECT 
        clients.name,
        SUM(sales.amount) as revenue
    FROM sales
    JOIN clients ON sales.client_id = clients.id
    GROUP BY clients.name
""")

# –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
analytics.export_to_excel(result, "report.xlsx")
analytics.export_to_parquet(result, "report.parquet")
```

### –û–±—Ä–∞–±–æ—Ç–∫–∞ PDF/DOCX –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (Docling)

```python
from scripts.processors.docling_processor import DoclingProcessor

processor = DoclingProcessor()

# –û–¥–∏–Ω —Ñ–∞–π–ª
markdown = processor.process_file(
    "financial_report.pdf",
    app_name="client-reports"
)

# –¶–µ–ª–∞—è –ø–∞–ø–∫–∞
results = processor.process_directory(
    "inbox/",
    app_name="client-reports"
)

# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤ S3:
# - raw/client-reports/financial_report.pdf
# - processed/client-reports/financial_report.md
# - metadata/client-reports/financial_report.json
```

### –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å —Å–∞–π—Ç–æ–≤ (Crawl4AI)

```python
from scripts.processors.crawler_crawl4ai import crawl_website
import asyncio

async def crawl():
    results = await crawl_website(
        start_url="https://company.com",
        max_pages=10,
        app_name="competitor-analysis"
    )
    return results

# –ó–∞–ø—É—Å–∫
results = asyncio.run(crawl())

# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ S3: crawled/competitor-analysis/
```

## üìä –†–∞–±–æ—Ç–∞ —Å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–û–°–í)

### –ú–æ–¥—É–ª—å Finance - –û–°–í –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è

**–ò–º–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –û–°–í:**
```bash
cd /opt/docagent
source venv/bin/activate

# –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –û–°–í –∏–∑ Excel
python scripts/finance/import_osv_improved.py

# –°–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
python scripts/finance/consolidated_report.py
```

**–ß–µ—Ä–µ–∑ DuckDB Analytics:**
```python
from scripts.analytics.duckdb_analytics import DuckDBAnalytics

# –ü–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –±–∞–∑–µ –û–°–í
analytics = DuckDBAnalytics(
    db_path="knowledge_base/duckdb/osv/osv_database.duckdb"
)

# –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –ø–æ –≤—Å–µ–º –∫–æ–º–ø–∞–Ω–∏—è–º
result = analytics.query("""
    SELECT 
        account_number,
        account_name,
        SUM(debit_turnover) as total_debit,
        SUM(credit_turnover) as total_credit
    FROM osv_data
    GROUP BY account_number, account_name
    ORDER BY account_number
""")

analytics.export_to_excel(result, "consolidated_osv.xlsx")
```

**–ü–æ–¥—Ä–æ–±–Ω–µ–µ:** `scripts/finance/README.md`

---

## üìä –¢–∏–ø–æ–≤—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏

### –°—Ü–µ–Ω–∞—Ä–∏–π 1: –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

```python
# –£ –≤–∞—Å –µ—Å—Ç—å:
# - sales_q1.xlsx
# - sales_q2.xlsx
# - clients_list.xlsx
# - products_catalog.csv

analytics = DuckDBAnalytics()

# –ò–º–ø–æ—Ä—Ç –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
analytics.import_excel("sales_q1.xlsx")
analytics.import_excel("sales_q2.xlsx") 
analytics.import_excel("clients_list.xlsx")
analytics.import_csv("products_catalog.csv")

# –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
result = analytics.query("""
    -- –û–±—ä–µ–¥–∏–Ω—è–µ–º Q1 –∏ Q2
    WITH all_sales AS (
        SELECT * FROM sales_q1
        UNION ALL
        SELECT * FROM sales_q2
    )
    -- –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–ª–∏–µ–Ω—Ç–∞—Ö –∏ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö
    SELECT 
        c.company_name,
        c.industry,
        p.product_name,
        p.category,
        COUNT(s.id) as deals,
        SUM(s.amount) as revenue,
        AVG(s.amount) as avg_deal
    FROM all_sales s
    JOIN clients_list c ON s.client_id = c.id
    JOIN products_catalog p ON s.product_id = p.id
    GROUP BY 1, 2, 3, 4
    ORDER BY revenue DESC
""")

# –≠–∫—Å–ø–æ—Ä—Ç
analytics.export_to_excel(result, "h1_2024_revenue_by_client.xlsx")
```

### –°—Ü–µ–Ω–∞—Ä–∏–π 2: –ê–Ω–∞–ª–∏–∑ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞

```python
# –ö–ª–∏–µ–Ω—Ç –ø—Ä–∏—Å–ª–∞–ª –≥–æ–¥–æ–≤–æ–π –æ—Ç—á–µ—Ç –Ω–∞ 150 —Å—Ç—Ä–∞–Ω–∏—Ü

processor = DoclingProcessor()

# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ Markdown
markdown = processor.process_file(
    "client_annual_report_2024.pdf",
    app_name="acme-corp"
)

# –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ:
# 1. –ò—Å–∫–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ —Ä–µ–≥—É–ª—è—Ä–∫–∞–º–∏
import re

# –ù–∞–π—Ç–∏ –≤—ã—Ä—É—á–∫—É
revenue_pattern = r'–≤—ã—Ä—É—á–∫–∞.*?(\d[\d\s,\.]+)\s*(–º–ª–Ω|–º–ª—Ä–¥|—Ç—ã—Å)'
revenues = re.findall(revenue_pattern, markdown, re.IGNORECASE)

# 2. –ò–∑–≤–ª–µ—á—å —Ç–∞–±–ª–∏—Ü—ã
tables = processor.extract_tables(markdown)

# 3. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
# (—Ç—Ä–µ–±—É–µ—Ç—Å—è PostgreSQL + —ç–º–±–µ–¥–¥–∏–Ω–≥–∏)
```

### –°—Ü–µ–Ω–∞—Ä–∏–π 3: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤

```python
# –ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å —Å–∞–π—Ç–æ–≤ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤

competitors = [
    ("https://competitor1.com", "competitor-1"),
    ("https://competitor2.com", "competitor-2"),
    ("https://competitor3.com", "competitor-3"),
]

async def monitor_all():
    for url, name in competitors:
        print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {name}...")
        results = await crawl_website(
            start_url=url,
            max_pages=5,
            app_name=f"monitoring/{name}"
        )
        print(f"  ‚úÖ –°–æ–±—Ä–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(results)}")

asyncio.run(monitor_all())

# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:
# - crawled/monitoring/competitor-1/...
# - crawled/monitoring/competitor-2/...
# - crawled/monitoring/competitor-3/...
```

### –°—Ü–µ–Ω–∞—Ä–∏–π 4: –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤

```python
# –ù–æ–≤—ã–π –∫–ª–∏–µ–Ω—Ç, –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ –∫–µ–π—Å—ã –∏–∑ –æ–ø—ã—Ç–∞

from sentence_transformers import SentenceTransformer
import psycopg2

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# –û–ø–∏—Å–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞
query = """
–†–∏—Ç–µ–π–ª, 50 –º–∞–≥–∞–∑–∏–Ω–æ–≤, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–∫—É–ø–æ–∫,
—Å–Ω–∏–∂–µ–Ω–∏–µ –Ω–µ–ª–∏–∫–≤–∏–¥–∞, –±—é–¥–∂–µ—Ç —Å—Ä–µ–¥–Ω–∏–π, 3 –º–µ—Å—è—Ü–∞
"""

# –ü–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥
query_vec = model.encode(query)

# –ò—Å–∫–∞—Ç—å –≤ PostgreSQL
conn = psycopg2.connect(
    host="localhost",
    database="docagent",
    user="docagent",
    password="docagent123"
)

cur = conn.cursor()
cur.execute("""
    SELECT 
        project_name,
        description,
        1 - (embedding <=> %s::vector) as similarity
    FROM past_projects
    WHERE 1 - (embedding <=> %s::vector) > 0.7
    ORDER BY similarity DESC
    LIMIT 5
""", (query_vec.tolist(), query_vec.tolist()))

for row in cur.fetchall():
    print(f"{row[0]}: {row[2]:.1%} –ø–æ—Ö–æ–∂–µ—Å—Ç—å")
```

## üîß –£—Ç–∏–ª–∏—Ç—ã

### –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL

```bash
docker ps | grep postgres
docker exec -it postgres-docagent psql -U docagent -d docagent -c "\dt"
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ S3

```python
from config.s3_config import list_s3_files

# –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ S3
files = list_s3_files(prefix="processed/")
for f in files[:10]:
    print(f)
```

### –û—á–∏—Å—Ç–∫–∞ DuckDB

```python
analytics = DuckDBAnalytics()

# –°–ø–∏—Å–æ–∫ —Ç–∞–±–ª–∏—Ü
tables = analytics.list_tables()
print(f"–¢–∞–±–ª–∏—Ü: {len(tables)}")

# –£–¥–∞–ª–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É
analytics.query("DROP TABLE IF EXISTS sales_2023")

# –ò–ª–∏ —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é –±–∞–∑—É
analytics = DuckDBAnalytics(db_path="knowledge_base/duckdb/new_analytics.duckdb")
```

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ S3

```
bucket/
‚îú‚îÄ‚îÄ raw/                    # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
‚îÇ   ‚îî‚îÄ‚îÄ {app_name}/
‚îÇ       ‚îî‚îÄ‚îÄ file.pdf
‚îú‚îÄ‚îÄ processed/              # –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ (Markdown)
‚îÇ   ‚îî‚îÄ‚îÄ {app_name}/
‚îÇ       ‚îî‚îÄ‚îÄ file.md
‚îú‚îÄ‚îÄ metadata/               # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ JSON
‚îÇ   ‚îî‚îÄ‚îÄ {app_name}/
‚îÇ       ‚îî‚îÄ‚îÄ file.json
‚îî‚îÄ‚îÄ crawled/               # –î–∞–Ω–Ω—ã–µ —Å —Å–∞–π—Ç–æ–≤
    ‚îî‚îÄ‚îÄ {app_name}/
        ‚îî‚îÄ‚îÄ page.md
```

## üîë –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è (.env)

```bash
# S3 Storage (Beget)
S3_BUCKET=your-bucket
S3_ACCESS_KEY=your-access-key
S3_SECRET_KEY=your-secret-key
S3_ENDPOINT=https://s3.ru1.storage.beget.cloud

# PostgreSQL
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=docagent
POSTGRES_USER=docagent
POSTGRES_PASSWORD=docagent123
```

## üìö –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- [–ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–∏—Å—Ç–∞](docs/FINANCIAL_QUICKSTART.md)
- [–ì–∞–π–¥ –ø–æ DuckDB](docs/DUCKDB_INTEGRATION.md)
- [–ì–∞–π–¥ –ø–æ Docling](docs/DOCLING_INTEGRATION.md)
- [–ì–∞–π–¥ –ø–æ Crawl4AI](docs/CRAWL4AI_GUIDE.md)

## üÜò –ß–∞—Å—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã

**DuckDB: —Ç–∞–±–ª–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞**
```python
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–ø–∏—Å–æ–∫ —Ç–∞–±–ª–∏—Ü
analytics.list_tables()
```

**Docling: ImportError**
```bash
# –ù—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π —Å—Ç–µ–∫
pip install -r requirements.full.txt
```

**S3: Access Denied**
```python
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ credentials –≤ .env
# –î–ª—è Beget –Ω—É–∂–Ω—ã –æ–±–∞ signature_version
```

**PostgreSQL: connection refused**
```bash
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
docker ps | grep postgres
# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ
docker restart postgres-docagent
```

---

**–£—Å–ø–µ—à–Ω–æ–π —Ä–∞–±–æ—Ç—ã!** üíºüìä
